✅ （1）GPT 是什麼？（超白話版）
GPT = 一個能「讀懂文字」＋「生成文字」的 AI。
拆開來就是：
G：Generative → 會自己生出內容
P：Pre-trained → 已經先讀過超大量資料
T：Transformer → 內部使用 Transformer 模型架構（目前最強的 NLP 架構）
簡單講：
GPT 是一種看過超大量資料後，能理解語言、回答問題、寫程式、翻譯、推理的 AI 模型。
像 ChatGPT 背後就是 GPT 模型。
🧠 （2）GPT 是什麼？（正式學術版）
GPT（Generative Pre-trained Transformer）是一種基於 Transformer 架構的大型語言模型（LLM），透過大量未標註資料進行自我監督式預訓練，學習語言的語意、語法與知識結構，再經由特定任務微調，使模型能完成文本生成、摘要、對話、程式撰寫、推理等自然語言任務。
GPT 的核心特色包含：
生成式能力（Generative）
能輸出一段合理連貫的語言內容。
預訓練（Pre-trained）
在海量文字資料上學習語言規律，之後可少量資料微調。
Transformer 架構（T）
利用 Self-attention 捕捉長距離語義，比 RNN 更有效率。
🎤 （3）口試 30 秒講稿版
你可以直接講：
「GPT 全名是 Generative Pre-trained Transformer，它是一種大型語言模型。GPT 先在超大量文字上做預訓練，學會語意、語法與知識結構，再透過微調來執行特定任務。因為它基於 Transformer 架構，所以能有效理解上下文，並產生連貫的語句。GPT 可以用在對話、摘要、翻譯、醫療文本分析、生成程式碼等多種 NLP 任務，是目前最主流的自然語言處理模型之一。」
如果教授再問
「那 GPT 的強項是什麼？」
你可以補一句：
「GPT 的強項是語言理解 + 語言生成，同時具備推理能力，可以在幾乎沒有任務特化的情況下做到泛用。」
